---
title: "PCA and K-means clustering"
date: 2018-01-20T18:49:07Z
tags: ["R", "Statistical Learning"]
draft: true
---
In this problem, we will generate simulated data, and then permorm PCA and K-means clustering on the data.

We first create cluster `c1` using `rnorm()` function and then,
by shifting the observations of that cluster,
we create clusters `c2` and `c3`.
Finally, using `rbind()` function, we combine all three clusters to a single data frame.

``` r
set.seed(1)
c1 <- matrix(rnorm(1000), ncol = 50)
c2 <- c1 + 2
c3 <- c1 - 3

data <- rbind(c1, c2, c3)
```

### PCA

We now perform *principal component analysis*
using the `prcomp()` function without scaling the data and
then plot the principal component score vectors and the clusters.

``` r
pr.out <- prcomp(data, scale = FALSE)

biplot(pr.out, scale = 0)
```

<img src="/images/PCA_2-1.png" style="display: block; margin: auto;" />

``` r
plot(pr.out$x,
     main = "Unscaled",
     xlab = "First Principal Component",
     ylab = "Second Principal Component")
points(pr.out$x[1:20, c(1,2)], pch = 20, col = "red")    
points(pr.out$x[21:40, c(1,2)], pch = 20, col = "green")
points(pr.out$x[41:60, c(1,2)], pch = 20, col = "blue")   
```

<img src="/images/PCA_2-2.png" style="display: block; margin: auto;" />

### K-means

We perform *K-means* clustering of the observations with K = 3.
As we can see, the model has perfectly clustered the data.

``` r
km.out.3 <- kmeans(data, 3, nstart = 20)
km.out.3$cluster
```

    ##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
    ## [36] 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

``` r
table(km.out.3$cluster)
```

    ##
    ##  1  2  3
    ## 20 20 20

We now perform K-means clustering of the observations with K = 2.
The results show that one of the clusters has been left untouched whereas
the rest two have been merged.

``` r
km.out.2 <- kmeans(data, 2, nstart = 20)
km.out.2$cluster
```

    ##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
    ## [36] 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

``` r
table(km.out.2$cluster)
```

    ##
    ##  1  2
    ## 20 40

When we apply a K-means model with K = 4,
we observe that
two of the generated clusters have 20 observations (as per original model)
whereas the third cluster has been split into two new clusters.

``` r
km.out.4 <- kmeans(data, 4, nstart = 20)
km.out.4$cluster
```

    ##  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 1 2 1 1 2 1 2 1 2 1 2 2 1
    ## [36] 1 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4

``` r
table(km.out.4$cluster)
```

    ##
    ##  1  2  3  4
    ##  9 11 20 20

We now perform K-means clustering of the observations with K = 3 on the first two principal components.
The generated clusters are identical to the one generated by the original model which was built using the raw data.

``` r
pca <- pr.out$x[, c(1,2)]
km.out.pca <- kmeans(pca, 3, nstart = 20)
km.out.pca$cluster
```

    ##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
    ## [36] 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3

``` r
table(km.out.pca$cluster)
```

    ##
    ##  1  2  3
    ## 20 20 20

We now perform K-means clustering of the observations with K = 3
using the scaled data with standard deviation one.
As we can see,
as expected,
the clusters have not been changed.

``` r
data.sc <- scale(data)

km.out.sc <- kmeans(data.sc, 3, nstart = 20)
km.out.sc$cluster
```

    ##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
    ## [36] 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3

``` r
table(km.out.sc$cluster)
```

    ##
    ##  1  2  3
    ## 20 20 20

*This analysis is based on Exercise 10.7.10 from "An Introduction to Statistical Learning" book by Robert Tibshirani and Trevor Hastie*
